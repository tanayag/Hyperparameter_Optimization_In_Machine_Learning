{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/greatskull/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1,               Epoch: 0,               Loss: 1.295329552901366\n",
      "Rank: 2,               Epoch: 0,               Loss: 1.296424038380197\n",
      "Rank: 0,               Epoch: 0,               Loss: 1.3036641458545342\n",
      "Rank: 1,               Epoch: 1,               Loss: 0.5478362081185827\n",
      "Rank: 0,               Epoch: 1,               Loss: 0.5478412276543911\n",
      "Rank: 2,               Epoch: 1,               Loss: 0.5375671426080307\n",
      "Rank: 0,               Epoch: 2,               Loss: 0.42336394725293236\n",
      "Rank: 2,               Epoch: 2,               Loss: 0.4188714563471716\n",
      "Rank: 1,               Epoch: 2,               Loss: 0.4258106874782334\n",
      "Rank: 2,               Epoch: 3,               Loss: 0.3696491805449972\n",
      "Rank: 0,               Epoch: 3,               Loss: 0.35901896839871594\n",
      "Rank: 1,               Epoch: 3,               Loss: 0.3639175708813737\n",
      "Rank: 2,               Epoch: 4,               Loss: 0.3265075064430197\n",
      "Rank: 0,               Epoch: 4,               Loss: 0.31880859651093213\n",
      "Rank: 1,               Epoch: 4,               Loss: 0.32762547351997856\n",
      "Rank: 0,               Epoch: 5,               Loss: 0.2885365558854564\n",
      "Rank: 1,               Epoch: 5,               Loss: 0.29234498530720016\n",
      "Rank: 2,               Epoch: 5,               Loss: 0.29812359074189226\n",
      "Rank: 0,               Epoch: 6,               Loss: 0.2708819454977098\n",
      "Rank: 1,               Epoch: 6,               Loss: 0.26713112734679906\n",
      "Rank: 2,               Epoch: 6,               Loss: 0.269506484027991\n",
      "Rank: 0,               Epoch: 7,               Loss: 0.25162798891027516\n",
      "Rank: 2,               Epoch: 7,               Loss: 0.25548025066778846\n",
      "Rank: 1,               Epoch: 7,               Loss: 0.2506237682521312\n",
      "Rank: 2,               Epoch: 8,               Loss: 0.23593593306792607\n",
      "Rank: 0,               Epoch: 8,               Loss: 0.23789295526826656\n",
      "Rank: 1,               Epoch: 8,               Loss: 0.23869857958965082\n",
      "Rank: 1,               Epoch: 9,               Loss: 0.22728494532109556\n",
      "Rank: 0,               Epoch: 9,               Loss: 0.229001600119584\n",
      "Rank: 2,               Epoch: 9,               Loss: 0.22557860721230694\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.autograd import Variable\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "import os\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Partition(object):\n",
    "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Network architecture. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "def partition_dataset():\n",
    "    \"\"\" Partitioning MNIST \"\"\"\n",
    "    transformations = [transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "                      ]\n",
    "    dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(transformations))\n",
    "    size = dist.get_world_size()\n",
    "    bsz = int(128 / float(size))\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        partition, batch_size=bsz, shuffle=True)\n",
    "    return train_set, bsz\n",
    "\n",
    "\n",
    "def average_gradients(model):\n",
    "    \"\"\" Gradient averaging. \"\"\"\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
    "        param.grad.data /= size\n",
    "\n",
    "\n",
    "def run(rank, size):\n",
    "    torch.manual_seed(1234)\n",
    "    train_set, bsz = partition_dataset()\n",
    "    model = Net()\n",
    "    model = model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_set:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            average_gradients(model)\n",
    "            optimizer.step()\n",
    "        print(f'Rank: {dist.get_rank()}, \\\n",
    "              Epoch: {epoch}, \\\n",
    "              Loss: {epoch_loss / num_batches}')\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 3\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
